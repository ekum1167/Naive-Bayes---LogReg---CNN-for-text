{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "project03_NBLR.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "G22YlnyxLmqd"
      },
      "source": [
        "#credentials for dataset\r\n",
        "import os\r\n",
        "from pydrive.auth import GoogleAuth\r\n",
        "from pydrive.drive import GoogleDrive\r\n",
        "from google.colab import auth\r\n",
        "from oauth2client.client import GoogleCredentials"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FxCosRt3Ub_z"
      },
      "source": [
        "auth.authenticate_user()\r\n",
        "gauth = GoogleAuth()\r\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\r\n",
        "drive = GoogleDrive(gauth)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Vr4iRu_V-qh"
      },
      "source": [
        "import pandas as pd"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QQ09SlCnUkXt",
        "outputId": "c93a310e-d278-4ffd-e375-fac5d270bd72"
      },
      "source": [
        "# DOWNLOAD LINKS\r\n",
        "link_train = 'https://drive.google.com/file/d/1JlCy6JQ5y6gBcMooJdMaWwc8XeMVIf52/view?usp=sharing'\r\n",
        "\r\n",
        "download = drive.CreateFile({'id': '1JlCy6JQ5y6gBcMooJdMaWwc8XeMVIf52'})\r\n",
        "download.GetContentFile('train.csv')\r\n",
        "\r\n",
        "df_train = pd.read_csv('train.csv')\r\n",
        "print(df_train)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "      Unnamed: 0  ...     label\n",
            "0              0  ...      spor\n",
            "1              1  ...  yazarlar\n",
            "2              2  ...     video\n",
            "3              3  ...     video\n",
            "4              4  ...      spor\n",
            "...          ...  ...       ...\n",
            "7995        7995  ...      spor\n",
            "7996        7996  ...      spor\n",
            "7997        7997  ...     dunya\n",
            "7998        7998  ...     dunya\n",
            "7999        7999  ...     dunya\n",
            "\n",
            "[8000 rows x 4 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xFOe9CDHWbME",
        "outputId": "d8077d22-15cc-4559-e48c-f04620fa9141"
      },
      "source": [
        "link_test = 'https://drive.google.com/file/d/1OrC0aelyv_Av90a5-PrmbaEquPRDmYP1/view?usp=sharing'\r\n",
        "\r\n",
        "\r\n",
        "download_test = drive.CreateFile({'id': '1OrC0aelyv_Av90a5-PrmbaEquPRDmYP1'})\r\n",
        "download_test.GetContentFile('test.csv')\r\n",
        "\r\n",
        "df_test = pd.read_csv('test.csv')\r\n",
        "print(df_test)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "      Unnamed: 0  ...     label\n",
            "0              0  ...     video\n",
            "1              1  ...     dunya\n",
            "2              2  ...     video\n",
            "3              3  ...     video\n",
            "4              4  ...  yazarlar\n",
            "...          ...  ...       ...\n",
            "1995        1995  ...      spor\n",
            "1996        1996  ...     dunya\n",
            "1997        1997  ...   turkiye\n",
            "1998        1998  ...  yazarlar\n",
            "1999        1999  ...      spor\n",
            "\n",
            "[2000 rows x 4 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t3ZgRkHUWokW",
        "outputId": "cfb2fad9-de78-4a31-9341-6300c156d1b8"
      },
      "source": [
        "# print shape\r\n",
        "print('Data Dimensionality: ' + str(len(df_train)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Data Dimensionality: 8000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NHIXgjCjbVF6",
        "outputId": "420ca0bf-3ab5-464a-9310-39cc3374ce20"
      },
      "source": [
        "import nltk\r\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AyIeQ3fbd816"
      },
      "source": [
        "train_data = df_train.text\r\n",
        "test_data = df_test.text"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iCjR3OXJZmNu"
      },
      "source": [
        "#Tokenizing text including preprocessing\r\n",
        "\r\n",
        "#Lowercased, stopwords removed\r\n",
        "from sklearn.feature_extraction.text import CountVectorizer\r\n",
        "from nltk.corpus import stopwords\r\n",
        "\r\n",
        "stopWords = set(stopwords.words('turkish'))\r\n",
        "stopWords.update(['ve', 'işte', 'bu', 'bir', 'bile' , 'var' , 'dedi' ,'onu', 'o','e','olan' ,'olarak', 'ancak' ,'kadar' ,'ın','n','da','artık','nın' ,'göre','ile','ise','için','değil', 'vs', 'acaba', 'ama','ayrıca','aslında','az','bazı','belki', 'biri' ,'birkaç', 'birşey', 'şey', 'biz', 'çok' ,'çünkü', 'daha' , 'de', 'defa', 'diye' , 'eğer' , 'en','gibi' , 'hem' , 'hep' , 'hepsi' , 'her' , 'hiç' , 'kez', 'ki', 'mı' , 'mi', 'mu' , 'mü' , 'nasıl' , 'ne' , 'neden' , 'nerde', 'nerede','nereye','niçin','niye','o','sanki','siz' , 'şu' , 'tüm' , 'veya' , 'ya' , 'yani'])\r\n",
        "vectorizer = CountVectorizer(lowercase=True, stop_words=stopWords) #CountVectorizer: Convert a collection of text documents to a matrix of token counts\r\n",
        "X = vectorizer.fit_transform(train_data)\r\n",
        "#print(vectorizer.get_feature_names())\r\n",
        "#print(X)\r\n",
        "X_test = vectorizer.fit_transform(test_data)\r\n"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B7rOFdrZblbb"
      },
      "source": [
        "#from sklearn.feature_extraction.text import TfidfTransformer\r\n",
        "\r\n",
        "#tfidf_transformer = TfidfTransformer()\r\n",
        "#X_train_tf = tfidf_transformer.fit_transform(X)\r\n",
        "\r\n",
        "#X_test_tf = tfidf_transformer.fit_transform(X_test)\r\n",
        "\r\n",
        "#print(X_train_tf)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jADjDDAGbpWF"
      },
      "source": [
        "#Splitting the data 80-20 for training and validation\r\n",
        "\r\n",
        "#train_data = X_train_tf[:4*(X_train_tf.shape[0]//5),:]\r\n",
        "#val_data = X_train_tf[4*(X_train_tf.shape[0]//5):,:]#\r\n",
        "\r\n",
        "#train_label = df_train.label[:4*(X_train_tf.shape[0]//5)]\r\n",
        "#val_label = df_train.label[4*(X_train_tf.shape[0]//5):]\r\n",
        "\r\n",
        "\r\n",
        "#print(\"Size of train data:\")\r\n",
        "#print(train_data.shape)\r\n",
        "\r\n",
        "#print(\"Size of val data:\")\r\n",
        "#print(val_data.shape)\r\n",
        "\r\n",
        "#print(\"Train data:\\n\", train_data)\r\n",
        "#print(\"Train label\\n\", train_label)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BpzrOvwvgVrn"
      },
      "source": [
        "import numpy as np, pandas as pd\r\n",
        "import seaborn as sns\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "from sklearn.datasets import fetch_20newsgroups\r\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\r\n",
        "from sklearn.naive_bayes import MultinomialNB\r\n",
        "from sklearn.pipeline import make_pipeline\r\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score\r\n",
        "from sklearn.model_selection import GridSearchCV"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vb1o78U2aciY"
      },
      "source": [
        "#Grid Search for Naive Bayes\r\n",
        "\r\n",
        "parameters_nb = {\r\n",
        "    #'tfidfvecorizer__max_df': (0.5,1, 2, 4),\r\n",
        "    #'vect__max_features': (None, 5000, 10000, 50000),\r\n",
        "    #'tfidf__ngram_range': ((1, 1), (1, 2), (1,3)),  # unigrams or bigrams or trigrams\r\n",
        "    #'tfidf__use_idf': (True, False),\r\n",
        "    #'tfidf__norm': ('l1', 'l2'),\r\n",
        "    #'clf__max_iter': (20, 40 ,50, 70, 80),\r\n",
        "    #'clf__alpha': (0.00001, 0.000001),\r\n",
        "    #'clf__penalty': ('l2', 'elasticnet'),\r\n",
        "    #'clf__max_iter': (10, 50, 80),4\r\n",
        "    #'vec__max_df': (0.5, 0.625, 0.75, 0.875, 1.0),  \r\n",
        "    #'vec__max_features': (None, 5000, 10000, 20000),  \r\n",
        "    #'vec__min_df': (1, 5, 10, 20, 50),  \r\n",
        "    #'tfidf__use_idf': (True, False),  \r\n",
        "    #'tfidf__sublinear_tf': (True, False),  \r\n",
        "    #'vec__binary': (True, False),  \r\n",
        "    #'tfidf__norm': ('l1', 'l2'),  \r\n",
        "    #'multinominalnb__alpha': (1, 0.1, 0.01, 0.001, 0.0001, 0.00001),\r\n",
        "    #'tfidfvectorizer__lowercase': (True, False),\r\n",
        "    #'alpha': [0.01, 0.1, 0.5, 1.0, 10.0],\r\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JpbdkZKT8enA"
      },
      "source": [
        "model = make_pipeline(TfidfVectorizer(), MultinomialNB())\r\n",
        "#print(\"paraters for model\", model.get_params)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gwan05tdaktM",
        "outputId": "3d440e86-a68c-49f8-acc6-b638a1b7a165"
      },
      "source": [
        "from time import time\r\n",
        "#Grid search for NAIVE BAYES\r\n",
        "\r\n",
        "grid_search = GridSearchCV(model, parameters_nb, n_jobs=-1, verbose=3)\r\n",
        "\r\n",
        "print(\"Performing grid search...\")\r\n",
        "print(\"pipeline:\", [name for name, _ in model.steps])\r\n",
        "print(\"parameters:\")\r\n",
        "print(parameters_nb)\r\n",
        "t0 = time()\r\n",
        "#print(grid_search.get_params().keys())\r\n",
        "grid_search.fit(df_train.text, df_train.label) \r\n",
        "print(\"done in %0.3fs\" % (time() - t0))\r\n",
        "print()\r\n",
        "\r\n",
        "print(\"Best score: %0.3f\" % grid_search.best_score_)\r\n",
        "print(\"Best parameters set:\")\r\n",
        "best_parameters_nb = grid_search.best_estimator_.get_params()\r\n",
        "for param_name in sorted(parameters_nb.keys()):\r\n",
        "    print(\"\\t%s: %r\" % (param_name, best_parameters_nb[param_name]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Performing grid search...\n",
            "pipeline: ['tfidfvectorizer', 'multinomialnb']\n",
            "parameters:\n",
            "{}\n",
            "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:    9.0s finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "done in 10.910s\n",
            "\n",
            "Best score: 0.717\n",
            "Best parameters set:\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TM6mrjkInHML"
      },
      "source": [
        "#There is no parameter to set for NB --> \r\n",
        "#https://www.reddit.com/r/MLQuestions/comments/6w196i/why_grid_search_is_not_performed_for_naive_bayes/\r\n",
        "# https://stats.stackexchange.com/questions/299842/why-grid-search-is-not-performed-for-naive-bayes-classifier\r\n",
        "\r\n",
        "\r\n",
        "# Train the model using the training data\r\n",
        "model.fit(df_train.text, df_train.label) \r\n",
        "# Predict the categories of the test data\r\n",
        "predicted_categories = model.predict(df_test.text)\r\n",
        "#predicted_categories = model.predict(X_test_tf)# error :AttributeError: lower not found"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gb-2OCFcpcNP",
        "outputId": "9c17638d-48d9-4478-f979-41fd154e2a9b"
      },
      "source": [
        "print(predicted_categories)\r\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['spor' 'yazarlar' 'video' ... 'turkiye' 'yazarlar' 'spor']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5mSh-C5DpreT",
        "outputId": "40d6c97c-87b2-4e87-a015-41f921b9b330"
      },
      "source": [
        "# plot the confusion matrix\r\n",
        "mat = confusion_matrix(df_test.label, predicted_categories)\r\n",
        "\r\n",
        "print(\"The accuracy is {}\".format(accuracy_score(df_test.label, predicted_categories)))\r\n",
        "print(classification_report(model.predict(df_test.label),predicted_categories ))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The accuracy is 0.7055\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       dunya       0.80      0.76      0.78       395\n",
            "        spor       0.89      0.94      0.92       384\n",
            "     turkiye       0.59      0.67      0.63       421\n",
            "       video       0.95      0.20      0.34       408\n",
            "    yazarlar       0.58      0.98      0.73       392\n",
            "\n",
            "    accuracy                           0.71      2000\n",
            "   macro avg       0.77      0.71      0.68      2000\n",
            "weighted avg       0.76      0.71      0.67      2000\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bTE6t5tdxhtF"
      },
      "source": [
        "# LOGISTIC REGRESSION PART\r\n",
        "from sklearn.pipeline import Pipeline\r\n",
        "from sklearn.linear_model import LogisticRegression\r\n",
        "from sklearn.feature_selection import SelectKBest, chi2\r\n",
        "\r\n",
        "pipeline = Pipeline([('vect', vectorizer),\r\n",
        "                     ('chi',  SelectKBest(chi2, k=1200)),\r\n",
        "                     ('clf', LogisticRegression(max_iter = 100000))])\r\n",
        "\r\n",
        "\r\n"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MYikqH9xgDqg"
      },
      "source": [
        "from sklearn.model_selection import GridSearchCV\r\n",
        "from sklearn.ensemble import RandomForestClassifier\r\n",
        "from sklearn.preprocessing import LabelEncoder\r\n",
        "import time"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QASMsNXsWitH"
      },
      "source": [
        "parameters = {\r\n",
        "    'vect__max_df': (0.5,1 , 2 , 4), #--> baska parametreler de ver.\r\n",
        "    'vect__max_features': (None, 5000, 10000),\r\n",
        "    'vect__ngram_range': ((1, 1), (1, 2)),  # unigrams or bigrams\r\n",
        "    #'tfidf__use_idf': (True, False),\r\n",
        "    #'tfidf__norm': ('l1', 'l2'),\r\n",
        "    'clf__max_iter': (20,40,50),\r\n",
        "    #'clf__alpha': (0.00001, 0.000001),\r\n",
        "    'clf__penalty': ('l1','l2', 'elasticnet'),\r\n",
        "    #'clf__max_iter': (10, 50, 80),\r\n",
        "    #'C':[0.001,.009,0.01,.09,1,5,10,25]\r\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9GrKI64MWkxn",
        "outputId": "dc96b0f5-8ca9-4f27-d04d-a89eb9f8657a"
      },
      "source": [
        "from time import time\r\n",
        "#Grid search for logistic regression\r\n",
        "grid_search = GridSearchCV(pipeline, parameters, n_jobs=-1, verbose=1)\r\n",
        "\r\n",
        "print(\"Performing grid search...\")\r\n",
        "print(\"pipeline:\", [name for name, _ in pipeline.steps])\r\n",
        "print(\"parameters:\")\r\n",
        "print(parameters)\r\n",
        "t0 = time()\r\n",
        "#print(grid_search.get_params().keys())\r\n",
        "grid_search.fit(df_train.text, df_train.label)\r\n",
        "print(\"done in %0.3fs\" % (time() - t0))\r\n",
        "print()\r\n",
        "\r\n",
        "print(\"Best score: %0.3f\" % grid_search.best_score_)\r\n",
        "print(\"Best parameters set:\")\r\n",
        "best_parameters = grid_search.best_estimator_.get_params()\r\n",
        "for param_name in sorted(parameters.keys()):\r\n",
        "    print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))\r\n",
        "\r\n",
        "#Since it will take time to finish this, here is the output for the last run\r\n",
        "\"\"\"\r\n",
        "Output:\r\n",
        "Performing grid search...\r\n",
        "pipeline: ['vect', 'chi', 'clf']\r\n",
        "parameters:\r\n",
        "{'vect__max_df': (0.5, 1, 2, 4), 'vect__max_features': (None, 5000, 10000), 'vect__ngram_range': ((1, 1), (1, 2)), 'clf__max_iter': (20, 40, 50), 'clf__penalty': ('l1', 'l2', 'elasticnet')}\r\n",
        "Fitting 5 folds for each of 216 candidates, totalling 1080 fits\r\n",
        "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\r\n",
        "[Parallel(n_jobs=-1)]: Done  46 tasks      | elapsed:  2.6min\r\n",
        "[Parallel(n_jobs=-1)]: Done 196 tasks      | elapsed: 11.7min\r\n",
        "[Parallel(n_jobs=-1)]: Done 446 tasks      | elapsed: 26.1min\r\n",
        "[Parallel(n_jobs=-1)]: Done 796 tasks      | elapsed: 46.8min\r\n",
        "[Parallel(n_jobs=-1)]: Done 1080 out of 1080 | elapsed: 63.9min finished\r\n",
        "done in 3841.845s\r\n",
        "\r\n",
        "Best score: 0.811\r\n",
        "Best parameters set:\r\n",
        "\tclf__max_iter: 20\r\n",
        "\tclf__penalty: 'l2'\r\n",
        "\tvect__max_df: 0.5\r\n",
        "\tvect__max_features: 5000\r\n",
        "\tvect__ngram_range: (1, 2)\r\n",
        "  STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\r\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Performing grid search...\n",
            "pipeline: ['vect', 'chi', 'clf']\n",
            "parameters:\n",
            "{'vect__max_df': (0.5, 1, 2, 4), 'vect__max_features': (None, 5000, 10000), 'vect__ngram_range': ((1, 1), (1, 2)), 'clf__max_iter': (20, 40, 50), 'clf__penalty': ('l1', 'l2', 'elasticnet')}\n",
            "Fitting 5 folds for each of 216 candidates, totalling 1080 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done  46 tasks      | elapsed:  2.6min\n",
            "[Parallel(n_jobs=-1)]: Done 196 tasks      | elapsed: 11.7min\n",
            "[Parallel(n_jobs=-1)]: Done 446 tasks      | elapsed: 26.1min\n",
            "[Parallel(n_jobs=-1)]: Done 796 tasks      | elapsed: 46.8min\n",
            "[Parallel(n_jobs=-1)]: Done 1080 out of 1080 | elapsed: 63.9min finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "done in 3841.845s\n",
            "\n",
            "Best score: 0.811\n",
            "Best parameters set:\n",
            "\tclf__max_iter: 20\n",
            "\tclf__penalty: 'l2'\n",
            "\tvect__max_df: 0.5\n",
            "\tvect__max_features: 5000\n",
            "\tvect__ngram_range: (1, 2)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xXmgHHG964_e",
        "outputId": "e7856cc9-f983-4e2d-ce5c-2abd33999454"
      },
      "source": [
        "# Using gridsearch parameters and applying to Log Res\r\n",
        "from sklearn.linear_model import LogisticRegression\r\n",
        "parameters = {\r\n",
        "  'clf__max_iter': 20,\r\n",
        "\t'clf__penalty': 'l2', #default is already l2 but explicitly define\r\n",
        "\t'vect__max_df': 0.5,\r\n",
        "\t'vect__max_features': 5000,\r\n",
        "\t'vect__ngram_range': (1, 2)\r\n",
        "}\r\n",
        "pipeline = Pipeline([('vect', vectorizer),\r\n",
        "                     ('chi',  SelectKBest(chi2, k=1200)),\r\n",
        "                     ('clf', LogisticRegression(max_iter = 100000))])\r\n",
        "pipeline.set_params(**parameters)\r\n",
        "pipeline.fit(df_train.text, df_train.label)\r\n",
        "#model.fit()"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Pipeline(memory=None,\n",
              "         steps=[('vect',\n",
              "                 CountVectorizer(analyzer='word', binary=False,\n",
              "                                 decode_error='strict',\n",
              "                                 dtype=<class 'numpy.int64'>, encoding='utf-8',\n",
              "                                 input='content', lowercase=True, max_df=0.5,\n",
              "                                 max_features=5000, min_df=1,\n",
              "                                 ngram_range=(1, 2), preprocessor=None,\n",
              "                                 stop_words={'acaba', 'ama', 'ancak', 'artık',\n",
              "                                             'aslında', 'ayrıca', 'az', 'bazı',\n",
              "                                             'belki', 'bile', 'bir', 'bir...\n",
              "                                 tokenizer=None, vocabulary=None)),\n",
              "                ('chi',\n",
              "                 SelectKBest(k=1200,\n",
              "                             score_func=<function chi2 at 0x7f3d3f1e70d0>)),\n",
              "                ('clf',\n",
              "                 LogisticRegression(C=1.0, class_weight=None, dual=False,\n",
              "                                    fit_intercept=True, intercept_scaling=1,\n",
              "                                    l1_ratio=None, max_iter=20,\n",
              "                                    multi_class='auto', n_jobs=None,\n",
              "                                    penalty='l2', random_state=None,\n",
              "                                    solver='lbfgs', tol=0.0001, verbose=0,\n",
              "                                    warm_start=False))],\n",
              "         verbose=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VlZ7jqwj0NMT",
        "outputId": "bed51c46-c261-416b-c3a4-1a20ac4d0059"
      },
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\r\n",
        "print(classification_report(df_test.label, pipeline.predict(df_test.text)))"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "       dunya       0.79      0.83      0.81       395\n",
            "        spor       0.91      0.87      0.89       384\n",
            "     turkiye       0.72      0.64      0.68       421\n",
            "       video       0.74      0.82      0.78       408\n",
            "    yazarlar       0.88      0.88      0.88       392\n",
            "\n",
            "    accuracy                           0.81      2000\n",
            "   macro avg       0.81      0.81      0.81      2000\n",
            "weighted avg       0.81      0.81      0.81      2000\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kBFzUezec78U"
      },
      "source": [
        "References:\r\n",
        "https://towardsdatascience.com/text-classification-using-naive-bayes-theory-a-working-example-2ef4b7eb7d5a\r\n",
        "\r\n",
        "https://medium.com/analytics-vidhya/applying-text-classification-using-logistic-regression-a-comparison-between-bow-and-tf-idf-1f1ed1b83640\r\n",
        "\r\n",
        "https://www.kaggle.com/shahkan/text-classification-using-logistic-regression\r\n",
        "\r\n",
        "https://towardsdatascience.com/logistic-regression-model-tuning-with-scikit-learn-part-1-425142e01af5"
      ]
    }
  ]
}